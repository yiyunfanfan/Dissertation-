{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pip install twython\n",
    "from twython import Twython\n",
    "import re, os, time\n",
    "import dateutil.parser as dateparser\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class edu_tweet_downloader(object):\n",
    "    ## initialize the Tweet downloader\n",
    "    def __init__(self, date, consumer_key, last, consumer_secret, MAX_LOOKUP_NUMBER=100, SLEEP_TIME=60,\n",
    "                 keywords=[\"teach\", \"educat\", \"school\", \"student\", \"university\", \"college\"]):\n",
    "\n",
    "        # accept user input\n",
    "        self.MAX_LOOKUP_NUMBER = MAX_LOOKUP_NUMBER\n",
    "        self.SLEEP_TIME = SLEEP_TIME\n",
    "        self.date = dateparser.parse(date).date()  \n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.keywords = [keyword.lower() for keyword in keywords]\n",
    "        self.last = last  # maximum number of Tweets to look up in a original daily dataset\n",
    "\n",
    "        ## initialize the API client\n",
    "        self.twitter = Twython(self.consumer_key, self.consumer_secret)\n",
    "\n",
    "        ## load a list of daily Tweet IDs to hydrate\n",
    "        self.tid_list = []\n",
    "        path = f\"./raw/{self.date}_clean-dataset.tsv\"\n",
    "        daily = pd.read_csv(path, header=0, sep='\\t')\n",
    "        self.tid_list = daily['tweet_id'][:int(last)].astype('str')\n",
    "\n",
    "        ## the total number of Tweets to hydrate\n",
    "        tid_number = len(self.tid_list)\n",
    "        ## compute the number of batches to download\n",
    "        max_round = tid_number // self.MAX_LOOKUP_NUMBER + 1\n",
    "        ## initialize time counters\n",
    "        now, then = 0, 0\n",
    "        ## loop over batches\n",
    "        self.statuses = []\n",
    "        for i in range(max_round):\n",
    "            ## slice out the Tweet ids for this batch\n",
    "            lookup_tids = self.tid_list[i * self.MAX_LOOKUP_NUMBER:\n",
    "                                        (i + 1) * self.MAX_LOOKUP_NUMBER]\n",
    "            ## advance time counters\n",
    "            then = now\n",
    "            now = time.time()\n",
    "            ## compute remaining time\n",
    "            REMAINING_SLEEP = self.SLEEP_TIME - int(now - then) + 1\n",
    "            ## only sleep if we have already made previous calls\n",
    "            if then:\n",
    "                time.sleep(REMAINING_SLEEP)\n",
    "            ## hydrate the daily Tweets\n",
    "            self.statuses.extend(\n",
    "                self.twitter.lookup_status(id=\",\".join(lookup_tids), tweet_mode='extended', trim_user='false'))\n",
    "            then = time.time()\n",
    "\n",
    "            ## filter the daily Tweets on the list and download the filtered\n",
    "\n",
    "    def download(self):\n",
    "        ## filter the Tweets with user-input keywords\n",
    "        filtered_ids = []\n",
    "        filtered_hashtags = []\n",
    "        filtered_statuses = []\n",
    "        for status in self.statuses:\n",
    "            if re.search('|'.join(self.keywords), status['full_text'].lower()) and status['lang'] == 'en':\n",
    "                filtered_ids.append(status['id'])\n",
    "                filtered_hashtags.extend([i['text'] for i in status['entities']['hashtags']])\n",
    "                filtered_statuses.append(status)\n",
    "        ##construct the database\n",
    "        os.system(\"mkdir './data/'\")\n",
    "        with open(f\"./data/{self.date}_ids.txt\", \"w\") as f:\n",
    "            for i in filtered_ids:\n",
    "                f.write(str(i) + \"\\n\")\n",
    "        with open(f\"./data/{self.date}_full.json\", \"w\") as f:\n",
    "            for status in filtered_statuses:\n",
    "                f.write(json.dumps(status) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example: download daily educational COVID Tweets from May 3rd, 2020\n",
    "downloader = edu_tweet_downloader('2020/'+str(5)+'/'+str(3), last=100000, consumer_key='...', \n",
    "                                  consumer_secret='...')\n",
    "downloader.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge daily files \n",
    "def merge_JsonFiles(filename, outputfilename): \n",
    "    data=[]\n",
    "    for f in filename: \n",
    "        file = open(f'./data/{f}_full.json',)     \n",
    "        for line in file: \n",
    "            data.append(json.loads(line))\n",
    "        file.close()\n",
    "    with open(f\"./data/{outputfilename}_full.json\", \"w\") as f: \n",
    "        for line in data: \n",
    "            f.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example: merge educational COVID Tweets from Phase 1 (May 3-9, 2020)\n",
    "filename=['2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09']\n",
    "merge_JsonFiles(filename, \"phase1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## json to excel \n",
    "df = pd.DataFrame()\n",
    "with open('./data/phase1_full.json', \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        result=json.loads(line)\n",
    "        l = [result]\n",
    "        df = df.append(pd.DataFrame(l), sort=True)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['created_at'] = df['created_at'].apply(lambda a: datetime.strftime(a,\"%Y-%m-%d %H:%M:%S\"))\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df.to_excel('./data/phase1_full.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example: read excel data from phase 1\n",
    "phase1data=pd.read_excel('./data/phase1_full.xlsx')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
